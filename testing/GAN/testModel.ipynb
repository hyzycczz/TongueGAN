{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just check the file is jpg and png or not, and rename it.\n",
    "\n",
    "PATH = \"./CastleDataset/\"\n",
    "file = os.listdir(PATH)\n",
    "for i in range(len(file)):\n",
    "    Ftype = file[i][-4:]\n",
    "    if(Ftype != \".jpg\" and Ftype != \".png\"):\n",
    "        continue\n",
    "    \n",
    "    os.rename(PATH+file[i], PATH+\"{:05d}\".format(i)+Ftype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.47891688]\n",
      " [0.6626682 ]\n",
      " [0.94325125]\n",
      " [0.60553527]\n",
      " [0.98789394]\n",
      " [0.14023149]\n",
      " [0.19815588]\n",
      " [0.7517792 ]\n",
      " [0.6913544 ]\n",
      " [0.3223511 ]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "data = tf.random.uniform((10,))\n",
    "data = data[:,None]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0f186faa00>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.0038016]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_4278/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 13:42:36.128370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-06 13:42:36.128585: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4278/2228458018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4278/3579287602.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Produce images for the GIF as you go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/minired/GraduationProject/Speech-denoise/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "941c29b67e2832d9853fcca68211864b6a3ce94cf618d80dbc84fcce13a7fd26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
