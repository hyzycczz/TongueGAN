# 沒看懂的地方
* p10 視窗的選取是重要的議題，....
# 重點整理
* 聲音的強度單位是分貝
* 聲音的響度代表主觀下表示聲音的大小，與頻率有關，單位有兩種。分別是"方(phon)" 與 "宋(sone)"。

## 傅立葉轉換前提
* 傅立葉轉換的假設前提是"訊號為非時變訊號"，也就是說輸入的音訊中的頻率不能隨著時間改變而改變。如下圖所示，(a)(b)分別是5Hz、15Hz、20Hz的混和音訊傅立葉前後的圖形，傅立葉轉換後可以明顯的看出音訊為5Hz、15Hz、20Hz。(c)(d)也是5Hz、15Hz、20Hz，但0.5秒以前為5Hz，0.5~1秒是15Hz，1秒後是20Hz，經過傅立葉轉換後圖形呈現(d)的樣子。
* 因為缺少了時間變化的資訊，因此無法觀測出全貌。
<p align='center'>
<img src='media\FT.png'>
</p>

* 因此，音窗就是為了解決這個問題而誕生的，也稱做短傅立葉轉換，一般音窗(window)會設在20ms~30ms之間，之後對每個音窗做快速傅立葉轉換。

## 漢明窗 ( Hamming Windows)
* 考慮到音窗的之間的關聯性與連續性，會使用Hamming window將音窗的兩邊降低，保留中間的音訊
* 為了更完整的還原短傅立葉轉換，音窗之間會重疊1/2

## 5.1 深度神經網路學習
利用深度學習的方式降噪似乎已經行之有年。此論文使用深層神經網路(deep neural network，DNN)模型作為基本架構。

將吵雜的語音以16ms的音窗做短時間傅立葉轉換，音框重疊範圍為1/2個音框，頻譜在第m個時間點表示為：
* $𝑦(𝑚)=[Y(1,𝑚),Y(2, 𝑚),…,Y(129,𝑚)]^T$
其中$Y(k,m)$為頻譜上上第 $m$ 個時間上且第 $k$ 個頻率的點，再將轉換後的絕對值取對數變成 $log(abs(y(m))$，這裡會將$abs(y(m))$後加入小偏差 0.01，目的是防止𝑙𝑜𝑔函數的輸出成無意義或縮小輸出值的範圍。

由於語音的連續性很強，所以將資料輸入深層神經網路的時候會前後取$L$個音框，也就是第$m\pm L$。

因硬體運算效能有限，所以每個音框只取的 256 點的快速傅立葉轉換，因實數傅立葉轉換後的振幅為偶函數，所以我們只取 256 點的一半和 DC 值，共 129 個點作為深層神經網路模型的輸入。

* 參考以下DNN降噪流程圖。
<p align='center'>
<img src='media\DNN流程圖.png'>
</p>

其中DNN的部分模型如下，模型架構共有五層，三層隱藏層中各有 500個神經元，並且加入 batch normalization 來防止收斂速度過慢和梯度爆炸的問題，因為是回歸模型，所以最後一層的激活函數用「linear」，而其他層皆用「Sigmoid function」。

<p align='center'>
<img src='media\DNN模型架構.png'>
</p>

論文上寫說，經過他們的實驗$L$的數值越大語音辨析度越高。但由於硬體關係，他們最後選擇$L=5$，因此總共會有看11個音窗。

* 經過DNN後，得出的乾淨語音的振幅頻譜對數量值，再將此估計值與吵雜語音頻譜的相位結合，做逆短時間傅立葉轉換，合成回時域的估計乾淨語音(predicted clean speech)。